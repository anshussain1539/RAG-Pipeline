# -*- coding: utf-8 -*-
"""RAG_PIPELINE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WC6sVV-0OM4tvZbHYj67Imnw8q3TlNtz

## Install Libraries
"""

!pip install sentence_transformers pypdf faiss-gpu
!pip install langchain langchain-openai
!pip install pypdf
!pip install sentence-transformers
!pip install langchain-together
!pip install langchain

"""## Import Libraries"""

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_together import Together
from langchain_together.embeddings import TogetherEmbeddings

"""## Load PDF"""

loader = PyPDFLoader("") ## give path of desired pdf (only pdf)
documents = loader.load()

"""## Split Document"""

text = RecursiveCharacterTextSplitter().split_documents(documents)

"""## Load Embedding Model"""

embeddings = TogetherEmbeddings(
                                  model="togethercomputer/m2-bert-80M-8k-retrieval" , ## you can change the embedding model
                                  together_api_key=""  ## write your together api key
                                )

"""## Create and store DB"""

# Create a vectorstore
vectorstore = FAISS.from_documents(text, embeddings)

# Save the documents and embeddings
vectorstore.save_local("vectorstore.db")

"""## Retrieve DB"""

# Create retriever
retriever = vectorstore.as_retriever()

"""## Load LLM"""

# from langchain_together import Together
def load_model(model_name="mistralai/Mixtral-8x7B-Instruct-v0.1"):
    llm = Together(
        model=model_name,
        temperature=0.7,
        max_tokens=128,
        top_k=1,
        together_api_key="" ## write your together api key
    )
    return llm

llm = load_model()

"""## System Prompt"""

# Define prompt template
template = """
You are an assistant for question-answering tasks.
Use the provided context only to answer the following question:

<context>
{context}
</context>

Question: {input}
"""

# Create a prompt template
prompt = ChatPromptTemplate.from_template(template)

"""## Create Chain"""

# Create a chain
doc_chain = create_stuff_documents_chain(llm, prompt)
chain = create_retrieval_chain(retriever, doc_chain)

"""## User Query"""

# User query
response = chain.invoke({"input": "Tell me about whole pdf."})

"""## Print Answer"""

# Get the Answer only
response['answer']